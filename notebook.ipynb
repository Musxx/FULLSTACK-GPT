{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI # í•˜ëŠ” ì—­í•  :ì±— ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain.prompts import ChatPromptTemplate # í•˜ëŠ” ì—­í•  :ì±— í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain.schema import BaseOutputParser # í•˜ëŠ” ì—­í•  :ì¶œë ¥ íŒŒì„œì˜ ê¸°ë³¸ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ì‚¬ìš©ì ì •ì˜ ì¶œë ¥ íŒŒì„œ ìƒì„±\n",
    "class CustomParser(BaseOutputParser): \n",
    "    def parse(self, text: str) -> str:\n",
    "        return text.split(\"\\n\")[0]\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ìƒì„±    \n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¤ìŒ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ìš”ì•½ ì²´ì¸ ìƒì„±\n",
    "summary_chain = summary_prompt | chat | CustomParser()\n",
    "# í•˜ëŠ” ì—­í•  : ìš”ì•½ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ëŒë‹¤ í•¨ìˆ˜ ìƒì„±\n",
    "summary_to_dict = lambda x: {\"summary\": x}\n",
    "# í•˜ëŠ” ì—­í•  : ë‹µë³€ í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ìƒì„±\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¤ìŒ ì§ˆë¬¸ì— ê°„ë‹¨íˆ ë‹µí•˜ì„¸ìš”.\"),\n",
    "    (\"human\", \"{summary}\")\n",
    "])\n",
    "# í•˜ëŠ” ì—­í•  : ë‹µë³€ ì²´ì¸ ìƒì„±\n",
    "answer_chain = answer_prompt | chat | CustomParser()\n",
    "# í•˜ëŠ” ì—­í•  : ì „ì²´ ì²´ì¸ ìƒì„±\n",
    "chain = summary_chain | summary_to_dict | answer_chain\n",
    "# í•˜ëŠ” ì—­í•  : ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "result = chain.invoke({\n",
    "    \"question\": \"íƒœì–‘ê³„ì—ëŠ” ì™œ í–‰ì„±ì´ 8ê°œì¸ê°€ìš”?\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate # í•˜ëŠ” ì—­í•  :ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "from langchain.prompts.prompt import PromptTemplate # í•˜ëŠ” ì—­í•  : í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector # í•˜ëŠ” ì—­í•  : ì˜ˆì œ ì„ íƒ ì‹œ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate  # í•˜ëŠ” ì—­í•  : ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±\n",
    "from langchain.globals import set_llm_cache # í•˜ëŠ” ì—­í•  : LLM ìºì‹œ ì„¤ì •\n",
    "from langchain.globals import set_debug # í•˜ëŠ” ì—­í•  : ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •\n",
    "from langchain.cache import InMemoryCache # í•˜ëŠ” ì—­í•  : ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ êµ¬í˜„\n",
    "from langchain.cache import SQLiteCache # í•˜ëŠ” ì—­í•  : SQLite ê¸°ë°˜ ìºì‹œ êµ¬í˜„\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : LLM ìºì‹œë¥¼ ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œë¡œ ì„¤ì •\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”\n",
    "set_debug(True)\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chat.predict(\"ìë°” ì‹¤ë¬´ ê°œë°œì— ëŒ€í•´ ì•Œë ¤ì¤˜.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8919176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ì‚¬ìš©ì ì •ì˜ ì¶œë ¥ íŒŒì„œ ìƒì„±\n",
    "class CustomParser(BaseOutputParser): \n",
    "    def parse(self, text: str) -> str:\n",
    "        return text.split(\"\\n\")[0]\n",
    "\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì¶œë ¥ íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Inception\",\n",
    "        \"output\": (\n",
    "            \"Title: Inception\\n\"\n",
    "            \"Director: Christopher Nolan\\n\"\n",
    "            \"Cast: Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page\\n\"\n",
    "            \"Budget: $160 million\\n\"\n",
    "            \"Box Office: $836.8 million\\n\"\n",
    "            \"Genre: Science Fiction, Thriller\\n\"\n",
    "            \"Synopsis: A skilled thief enters people's dreams to steal secrets, \"\n",
    "            \"but is offered a chance to erase his past crimes by planting an idea instead.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Movie: {input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  : ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ìµœì¢… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a movie information assistant. Always use the exact same format.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"Movie: {movie_name}\")\n",
    "])\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ìµœì¢… ì²´ì¸ ìƒì„±\n",
    "chain = final_prompt | chat | CustomParser()\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "result = chain.invoke({\n",
    "    \"movie_name\": \"Interstellar\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ae4ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹µë³€: âš¡ï¸ğŸ§™â€â™‚ï¸ğŸ”®\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate,FewShotChatMessagePromptTemplate, MessagesPlaceholder ,ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì±— ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ëŒ€í™” ìš”ì•½ ë²„í¼ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50, return_messages=True)\n",
    "\n",
    "def load_history(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì´ëª¨ì§€ í‘œí˜„ì„ ìœ„í•œ ì˜ˆì œë“¤ ì •ì˜\n",
    "examples = [\n",
    "    {\"question\": \"ëŒ€ë¶€\", \"answer\": \"ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\"},\n",
    "    {\"question\": \"ë°˜ì§€ì˜ ì œì™•\", \"answer\": \"ğŸ¦¸â€â™‚ï¸ğŸ§™â€â™‚ï¸ğŸ’\"}\n",
    "]\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"ì§ˆë¬¸: {question}\"),\n",
    "    (\"ai\", \"ë‹µë³€: {answer}\")\n",
    "])\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ëª‡ ê°€ì§€ ì˜ˆì œë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ìµœì¢… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"í•­ìƒ ì •í™•íˆ ì„¸ ê°œì˜ ì´ëª¨ì§€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    prompt,\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì „ì²´ ì²´ì¸ ìƒì„±\n",
    "chain = RunnablePassthrough.assign(history=load_history) | final_prompt | llm\n",
    "\n",
    "question = \"í•´ë¦¬í¬í„°\"\n",
    "\n",
    "# í•˜ëŠ” ì—­í•  :ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "result = chain.invoke({\n",
    "    \"question\": question\n",
    "})\n",
    "# í•˜ëŠ” ì—­í•  :ê²°ê³¼ ì¶œë ¥\n",
    "print(result.content)\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": question},\n",
    "    {\"output\": result.content}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
